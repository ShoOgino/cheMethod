  @Override
  protected void internalStart(Map<String, String> startOptions) throws InfrastructureException {
    KubernetesRuntimeContext<E> context = getContext();
    String workspaceId = context.getIdentity().getWorkspaceId();
    try {
      startSynchronizer.setStartThread();
      startSynchronizer.start();

      namespace.cleanUp();
      provisionWorkspace(startOptions, context, workspaceId);

      volumesStrategy.prepare(
          context.getEnvironment(),
          context.getIdentity(),
          startSynchronizer.getStartTimeoutMillis(),
          startOptions);

      startSynchronizer.checkFailure();

      startMachines();
      watchLogsIfDebugEnabled(startOptions);

      previewUrlCommandProvisioner.provision(context.getEnvironment(), namespace);
      runtimeStates.updateCommands(context.getIdentity(), context.getEnvironment().getCommands());

      startSynchronizer.checkFailure();

      final Map<String, CompletableFuture<Void>> machinesFutures = new LinkedHashMap<>();
      // futures that must be cancelled explicitly
      final List<CompletableFuture<?>> toCancelFutures = new CopyOnWriteArrayList<>();
      final EnvironmentContext currentContext = EnvironmentContext.getCurrent();
      CompletableFuture<Void> startFailure = startSynchronizer.getStartFailure();
      Span waitRunningAsyncSpan = tracer.buildSpan(WAIT_MACHINES_START).start();
      try (Scope waitRunningAsyncScope = tracer.scopeManager().activate(waitRunningAsyncSpan)) {
        TracingTags.WORKSPACE_ID.set(waitRunningAsyncSpan, workspaceId);
        for (KubernetesMachineImpl machine : machines.getMachines(context.getIdentity()).values()) {
          String machineName = machine.getName();
          final CompletableFuture<Void> machineBootChain =
              waitRunningAsync(toCancelFutures, machine)
                  // since machine running future will be completed from the thread that is not from
                  // kubernetes pool it's needed to explicitly put the executor to not to delay
                  // processing in the external pool.
                  .thenComposeAsync(checkFailure(startFailure), executor)
                  .thenRun(publishRunningStatus(machineName))
                  .thenCompose(checkFailure(startFailure))
                  .thenCompose(setContext(currentContext, checkServers(toCancelFutures, machine)))
                  .exceptionally(publishFailedStatus(startFailure, machineName));
          machinesFutures.put(machineName, machineBootChain);
        }
        waitMachines(machinesFutures, toCancelFutures, startFailure);
      } finally {
        waitRunningAsyncSpan.finish();
      }

      startSynchronizer.complete();
    } catch (InfrastructureException | RuntimeException e) {
      Exception startFailureCause = startSynchronizer.getStartFailureNow();
      if (startFailureCause == null) {
        startFailureCause = e;
      }

      startSynchronizer.completeExceptionally(startFailureCause);
      LOG.warn(
          "Failed to start Kubernetes runtime of workspace {}. Cause: {}",
          workspaceId,
          startFailureCause.getMessage());
      boolean interrupted =
          Thread.interrupted() || startFailureCause instanceof RuntimeStartInterruptedException;
      // Cancels workspace servers probes if any
      probeScheduler.cancel(workspaceId);
      // stop watching before namespace cleaning up
      namespace.deployments().stopWatch(true);
      try {
        namespace.cleanUp();
      } catch (InfrastructureException cleanUppingEx) {
        LOG.warn(
            "Failed to clean up namespace after workspace '{}' start failing. Cause: {}",
            context.getIdentity().getWorkspaceId(),
            cleanUppingEx.getMessage());
      }

      if (interrupted) {
        throw new RuntimeStartInterruptedException(getContext().getIdentity());
      }
      wrapAndRethrow(startFailureCause);
    } finally {
      namespace.deployments().stopWatch();
    }
  }

